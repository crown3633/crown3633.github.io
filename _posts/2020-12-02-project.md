---
layout: post
title:  "AIXDL - Project"
summary: "차량 공유 서비스의 차량 파손 여부를 CNN을 활용하여 판단"
author: taehun
date: '2020-12-02 16:53:00 +0900'
category: Python
toc : true
toc_sticky : true
thumbnail: /assets/img/posts/thumbnail-1.png
keywords: AI+X:DeepLearning, Dataset, VGGNet, Pre-trained model, ImageNet
permalink: /blog/project
---
----------
## **Index**
----------
<a href="#Introduction">**1. Introduction**</a>
- 필요성
- 목표<br><br>
 
<a href="#Dataset">**2. Dataset**</a>
- Gate1. Dataset
- Gate2. Dataset <br><br>
 
<a href="#VGGNet">**3. CNN / VGGNet**</a>
- CNN
- VGG16 <br><br>

<a href="#Code">**4. Code Analysis**</a> <br>
- <a href="#Gate1">Gate1. 이미지 데이터의 차량 여부 판단(Pre-trained VGG16 model)</a> <br>

- <a href="#Gate2">Gate2. 차량의 파손 여부 판단(Transfer Learning & Fine Tuning)</a> <br><br>

<a href="#Evaluation">**5. Evaluation**</a>
- Gate1. Evaluation
- Gate2. Evaluation <br><br>

<a href="#Discussion">**6. Discussion**</a> <br><br>

<a href="#Reference">**7. Reference**</a> <br><br>

-----------
## <a name="Introduction">1. Introduction</a>
-----------

#### 1) 필요성
 - 미래 모빌리티 산업에서 공유 서비스는 핵심 개념 중 하나이다. 대표적인 예로 '쏘카', '그린카' 등의 차량 공유 서비스뿐만 아니라, '따릉이','beam','씽씽'과 같은 자전거, 전동 킥보드의 공유 서비스도 있다. 공유서비스의 대상인 차량, 킥보드 등은 단독으로 소유하는 소유물이 아니기에, 파손 및 손상 여부에 대한 과실 판정을 제 때에, 정확하게 할 필요가 있다.<br><br> 렌트카의 경우 대여 및 반납시 대여자가 차량의 파손여부를 직접 체크한다. 차량 확인 과정에서 발견하지 못한, 즉 대여자의 과실이 아닌 손상으로 인해 발생하는 문제를 확실히 해결하기 위해서는 적절한 시스템이 필요할 것이다. 이 문제점을 해결하기 위해서 딥러닝의 이미지 분류를 이용해서 해결해보고자 한다.<br><br>

#### 2) 목표
 - 본 프로젝트에서는 딥러닝의 Image Classification을 활용해서 문제를 해결하고자 한다. 임의의 이미지 데이터를 입력했을 때 차량인지 아닌지를 판단하고, 만약 차량이 맞다면 차량의 파손 유무를 판단하는 것이 프로젝트의 최종 목표이다. <br><br> 판단은 2개의  Gate를 통해 이루어진다. Gate1에서는  Pre-trained model(VGGNet )을 활용하여 차량 여부 및 차량의 종류를 판단하고, Gate2에서는 직접 학습시킨 모델로  파손 여부를 판단한다.<br><br>
 
<div style="width:700px; height:300px;">
 <img src="/assets/img/Gate.png" width="700px" height="300px">
 <figcaption style="text-align:center">모델 알고리즘 개략도</figcaption>
</div><br><br>
 
----------------
## <a name="Dataset">2. Dataset</a>
----------------

#### - Gate1. Dataset
- data1 폴더 : Normal/Damaged 차량에 대한 이미지 파일 각 500개, 총 1000개(Gate1)
- data2 폴더 : Training data 600개(Normal/Damaged 각 300개)<br>　　 　　　　Validation data 200개(Normal/Damaged 각 100개)(Gate2)<br>

<div style="width:300px; height:300px; float:left; margin-right:10px;">
 <img src="/assets/img/normal.jpg" width="300px" height="300px">
 <figcaption style="text-align:center">Normal car image</figcaption>
</div>
<div style="width:300px; height:303px; float:left;">
 <img src="/assets/img/damaged.jpg" width="300px" height="303px">
 <figcaption style="text-align:center">Damaged car image</figcaption>
</div><div style="clear:both;"></div><br><br>

#### - Gate2. Dataset

<br><br>

-------------------
## <a name="VGGNet">3. CNN / VGGNet</a>
-------------------
<img src="/assets/img/VGGNet.png" width="700px" height="300px"><br><br>

-----------------
## <a name="Code">4. Code Analysis</a>
-----------------

#### <a name="Gate1">Gate1. 이미지 데이터의 차량 여부 판단(Pre-trained VGG16 model)</a>
- Gate 1은 Normal/Damaged car를 구분하기 전에 Input image가 차량인지 아닌지를 구별하기 위함<br><br>
##### **1)get_predictions(preds,top=5) function**

```python
def get_predictions(preds, top=5):
    global CLASS_INDEX
    if len(preds.shape) != 2 or preds.shape[1] != 1000:
        raise ValueError('`decode_predictions` expects '
                         'a batch of predictions '
                         '(i.e. a 2D array of shape (samples, 1000)). '
                         'Found array with shape: ' + str(preds.shape))
    if CLASS_INDEX is None:
        fpath = get_file('imagenet_class_index.json',
                         CLASS_INDEX_PATH,
                         cache_subdir='models')
        CLASS_INDEX = json.load(open(fpath))
    results = []
    for pred in preds:
        top_indices = pred.argsort()[-top:][::-1]
        result = [tuple(CLASS_INDEX[str(i)]) + (pred[i],) for i in top_indices]
        list_A = []
        for k in range(top):
        	list_A.append(result[i][1:])
        result.sort(key=lambda x: x[2], reverse=True)
        results.append(result)
    return results
```
```python
vgg16 = VGG16(weights='imagenet')

y = prepare_image('image_file.jpg')
preds = vgg16.predict(y)
print(get_predictions(preds, top=5))
```
- get_predictions(preds, top=5) 결과<br>
`[('racer', 0.6540158), ('sports_car', 0.26624563), ('convertible', 0.028868116), ...]`<br>

- 변수 preds는 data1 폴더의 dataset을 resizing 후 VGG16(weight='imagenet')으로 예측한 결과물이며, 1000개의 class에 대한 확률값을 나타내기에 차원은 (1,1000)이다.
- get_prediction 함수는 예측한 class 중 상위 5개의 class name과 확률값을 반환한다.<br><br>

##### **2)get_car_categories() function**
```python
def get_car_categories():
    d = defaultdict(float)
    img_list = os.listdir("data1")
    for i, img_path in enumerate(img_list):
        img = prepare_image(root_dir + "data1/"+ img_path)
        out = vgg16.predict(img)
        top = []
        # for i in range(5):
        # 	top.append(get_predictions(out,top=5)[0][i][1:])
        top = get_predictions(out, top=5)
        for j in top[0]:
            d[j[0:2]] += j[2]
            #d[j[1:2]] += j[2]
        if i % 100 == 0:
            print(i, "/" , len(img_list), "complete")
    print("Done\n")
    return Counter(d)
```  
- 해당 디렉토리에 있는 input 이미지에 대해 top5 class와 probability를 구해서 누적 합산하는 함수.
- top[0]에는 3가지 성분이 있음. 1000개 class중 고유 이름(j[0] : ex)'n5324', j[1] : class의 이름(label) ex)'sports car', j[2] : 확률값 ex)0.938402)
- 결국, 디렉토리 내에 있는 이미지 파일들 중 가장 빈번하게 등장하는 class들을 찾기 위함<br><br>

```python
car_categories_all = get_car_categories()
car_categories = [k for k, v in car_categories_all.most_common()[:50]]
```

- get_car_categories로 얻은 list중 key값이 큰 순서대로 상위 50개를 선택하여 car_categories에 저장<br>
`[('minivan',129.24480718624545), ('sports_car', 85.58487360691652), ('convertible', 71.450213502687), ('pickup', 62.12108225026168), ...]`
<figcaption style="text-align:center">위와 같은 형태로 50개가 출력됨</figcaption><br><br>

##### **3)evaluate_car_categories(car_categories) function**
```python
def evaluate_car_categories(car_categories):
    img_list = os.listdir("data1")
    num = 0
    failed_img = []
    for i, img_path in enumerate(img_list):
        img = prepare_image("data1/"+img_path)
        out = vgg16.predict(img)
        if i==1: print(out.shape)
        top = get_predictions(out, top=5)
        for j in top[0]:
            if j[0:2] in car_categories:
                num += 1
                break # breaks out of for loop if one of top 50 categories is found
            else:
                pass
            failed_img.append(img_path) # appends to "bad list" if none of the 50 are found
        if i % 100 == 0:
            print(i, "/", len(img_list), "complete")
    failed_img = [k for k, v in Counter(failed_img).items() if v == 5]
    return num, failed_img
```

- evaluate_car_categories 함수는 위에서 한번 분류한 car_categories를 가지고 failed_img를 찾아내는 함수
- input image에 대한 top5 prediction 중 어느 것도 car_categories안에 포함되지 않는다면 failed_img 리스트에 저장된다.

```python
print(failed_img)
```

　 `['1044.jpg', '1205.jpg', '1102.jpg', '1354.jpg']`
<br><br>
- **Failed_img 확인**
<div style="width:200px; height:300px; float:left; margin-right:10px;">
 <img src="/assets/img/failed_img1.png" width="200px" height="300px">
</div>
<div style="width:200px; height:300px; float:left;">
 <img src="/assets/img/failed_img2.png" width="200px" height="300px">
</div><div style="clear:both;"></div><br>

- 총 1,000개의 dataset에서 4개의 경우만 차량으로 인식하지 못한 것이므로 높은 정확도를 가진다.
- 하지만 극히 드물게 failed_img와 같이 여러 사물이 있는 경우, 혹은 차량의 일부만 나온 경우는 인식하지 못하는 경우가 있다.<br>
--> **Pre-trained VGG16을 이용해서 Nomal/Damaged car image 모두 높은 확률로 차량을 구별할 수 있다.** <br><br>

#### <a name="Gate2">Gate2. 차량의 파손 여부 판단(Transfer Learning & Fine Tuning)</a><br>

- 앞서서, Gate2에는 train_binary_model과 finetune_binary_model이 존재한다. train_binary_model은 convolutional layer는 pre-trained VGG16 model을 따르고, bottleneck features를 이용하여 fully-connetecd layer 부분의 weight만 조정해서 training을 진행하는 모델이며, fintune model은 pre-trained VGG16 model의 weight는 이용하되, train_binary_model에서 training을 거친 fully-connected layer의 weight를 가져와서, fully-connetecd layer뿐만 아니라 전체 layer에 대한 weight값을 dataset에 맞게 조정하는 모델을 의미한다.<br>

##### **1) load_vgg16() function**
  
```python
def load_vgg16():
    vgg16 = VGG16(weights='imagenet')
    model = Sequential()
    model.add(ZeroPadding2D((1,1),input_shape = (img_width, img_height,3)))
    model.add(Conv2D(64, (3, 3), padding = 'same', activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Conv2D(64, (3, 3), padding = 'same',activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))

    ...

    model.add(ZeroPadding2D((1,1)))
    model.add(Conv2D(512, (3, 3), padding = 'same',activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Conv2D(512, (3, 3), padding = 'same',activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Conv2D(512, (3, 3), padding = 'same',activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))
    
    #Transfer Learning 과정.
    model_conv_layer=[]
    vgg16_conv_layer=[]

    for j in range(len(model.layers)):
      if str(model.layers[j]).find("Conv2D")!=-1:
        model_conv_layer.append(j)

    for k in range(len(vgg16.layers)):
      if str(vgg16.layers[k]).find("Conv2D")!=-1:
        vgg16_conv_layer.append(k)

    for j, k in zip(model_conv_layer,vgg16_conv_layer):
      model.layers[j].set_weights(vgg16.layers[k].get_weights())
      
    return model
```

- VGG16 model은 Convolutional layer와 Fully connected layer를 합쳐서 16-layers이며, Padding, Pooling layer의 배치로 다양한 모델을 만들 수 있다.
- 아래 Transfer Learning 과정은 Pre-trained VGG16 model에서 Convolutional layer의 weight를 load_vgg16()을 통해 쌓은 model의 Convolutional layer에 weight를 인가하는 과정이다.<br>

##### **2) train_binary_model(batch_size) function**
  
```python
def train_binary_model(batch_size):

    train_data = np.load(open(location+"/bottleneck_features_train.npy",'rb'))
    train_labels = np.array([0] * train_samples[0] + 
                            [1] * train_samples[1])

    validation_data = np.load(open(location+"/bottleneck_features_validation.npy",'rb'))
    validation_labels = np.array([0] * validation_samples[0] + 
                                 [1] * validation_samples[1])
    # batch_size = 16
    
    model = Sequential()
    model.add(Flatten(input_shape=train_data.shape[1:])) # 512, 4, 4
    print(model.summary())
    model.add(Dense(256, activation = 'relu', kernel_regularizer=l2(0.001)))
    model.add(Dropout(0.5)) 
    model.add(Dense(1, activation = 'sigmoid')) # should activation be sigmoid for binary problem?

    model.compile(optimizers.SGD(lr=0.0001, momentum=0.9),
              loss='binary_crossentropy', metrics=['accuracy'])

    
    checkpoint = ModelCheckpoint(top_model_weights_path, monitor='val_accuracy', 
                                 verbose=1, save_best_only=True, save_weights_only=True, mode='auto')

    fit = model.fit(train_data, train_labels,
                    epochs = nb_epoch, batch_size = batch_size,
                    validation_data = (validation_data, validation_labels),
                    callbacks=[checkpoint])

    with open(location+'/top_history.txt', 'w') as f:
        json.dump(fit.history, f)
    
    return model, fit.history
```

- bottleneck_features는 VGG16 model에서 Fully-connected layer로 넘어가기 바로 전 Feature map을 의미한다.
- load_vgg16으로 만든 모델에 fully-connected layer를 연결하고 이 부분을 optimizer와 cross_entropy를 통해 학습시키는 것이다.<br><br>

##### **3) finetune_binary_model(batch_size) function**

```python
def finetune_binary_model(batch_size):
    model = load_vgg16()

    # build a classifier model to put on top of the convolutional model
    top_model = Sequential()
    top_model.add(Flatten(input_shape=model.output_shape[1:]))
    top_model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.01)))
    top_model.add(Dropout(0.5))
    top_model.add(Dense(1, activation='sigmoid'))

    top_model.load_weights(top_model_weights_path) # load weights_path

    # add the model on top of the convolutional base
    model.add(top_model)
    
    # set the first 25 layers (up to the last conv block)
    # to non-trainable - weights will not be updated
    for layer in model.layers[:25]:
        layer.trainable=False

    # compile the model with a SGD/momentum optimizer 
    # and a very slow learning rate
    model.compile(loss='binary_crossentropy',
                 optimizer = optimizers.SGD(lr=0.00001, momentum=0.9), # reduced learning rate by 1/10
                  metrics=['accuracy'])
    
    # prepare data augmentation configuration
    train_datagen = ImageDataGenerator(rescale=1./255,
                                       rotation_range=40,
                                       width_shift_range=0.2,
                                       height_shift_range=0.2,
                                       shear_range=0.2,
                                       zoom_range=0.2,
                                       horizontal_flip=True,
                                       fill_mode='nearest')

    test_datagen = ImageDataGenerator(rescale=1./255)

    train_generator= train_datagen.flow_from_directory(train_data_dir,
                                                     target_size=(img_height, img_width),
                                                     batch_size=batch_size,
                                                     class_mode='binary')

    validation_generator = test_datagen.flow_from_directory(validation_data_dir,
                                                           target_size=(img_height, img_width),
                                                           batch_size=batch_size,
                                                           class_mode='binary')
                                                           
    checkpoint = ModelCheckpoint(fine_tuned_model_path, monitor='val_accuracy', 
                                 verbose=1, save_best_only=True, 
                                 save_weights_only=False, mode='auto')
    steps_tr = nb_train_samples / batch_size
    steps_val = nb_validatioan_samples / batch_size
    # fine-tune the model
    fit = model.fit_generator(train_generator,
                              steps_per_epoch=steps_tr,
                              epochs=nb_epoch,
                              validation_data=validation_generator,
                              validation_steps=steps_val,
                              verbose=1,
                              callbacks=[checkpoint])
  
    with open(location+'/ft_history.txt', 'w') as f:
        json.dump(fit.history, f)
    
    return model, fit.history
```
###### **4) evaluate_binary_model(model, directory, labels, batch_size) function**
```python
def evaluate_binary_model(model, directory, labels, batch_size):
    datagen = ImageDataGenerator(rescale=1./255)
    generator = datagen.flow_from_directory(directory,
                                target_size=(img_height, img_width),
                                batch_size=batch_size,
                                class_mode='binary', # categorical for multiclass
                                shuffle=False)
    steps_ev = len(labels)/batch_size
    predictions = model.predict_generator(generator, steps_ev)
    
    # use for multiclass
    # pred_labels = np.argmax(predictions, axis=1)
    
    pred_labels = [0 if i <0.5 else 1 for i in predictions]

    print(classification_report(validation_labels, pred_labels))
    cm = confusion_matrix(validation_labels, pred_labels)
    sns.heatmap(cm, annot=True, fmt='g');
```

-----------------
## <a name="Evaluation">5. Evaluation</a>
-----------------
#### - Gate1. Evaluation

#### - Gate2. Evaluation

- 먼저, 최종 모델을 확정하기 전에 성능에 영향을 끼치는 hyperparameter를 조정해보자.<br>이 프로젝트에서 조정할 수 있는 hyperparameter는 epoch, batch size, learning rate, activation function, optimizer의 종류, momentum 등이 있다.<br>하지만 여기서는 epoch와 batch size만 조정하면서 테스트 해보았다. <br><br>
1) Epoch
먼저 아래 train_binary_model의 epoch에 따른 그래프를 비교해보자.
<div style="width:400px; height:200px; float:left; margin-right:10px">
 <img src="/assets/img/batch16_epoch50.png" width="400px" height="200px">
 <figcaption style="text-align:center">fig1)batch16_epoch50</figcaption>
</div>
<div style="width:400px; height:200px; float:left;">
 <img src="/assets/img/batch16_epoch100.png" width="400px" height="200px">
 <figcaption style="text-align:center">fig2)batch16_epoch100</figcaption>
</div><div style="clear:both;"></div><br>
- 위 그래프는 batch를 16으로 고정하고 epoch를 50, 100으로 변화시킨 그래프이다. <br>위 그래프뿐만 아니라 여러 batch size에서 epoch에 따른 변화를 관찰 했을 때 train binary model은 epoch가 20만 돼도 충분한 accuracy를 가지며, 그 이상으로 진행할 경우 train accuracy와 validation accuracy의 차이가 벌어지는, overfitting이 발생하는 것을 볼 수 있다.<br> 결과적으로 train binary model에서는 epoch = 20으로도 90% 정도의 accuracy를 갖는 결과를 낸다.<br><br>

2) Batch size
- 위 두 그래프는 train_binary_model에 대한 그래프이고, 아래 두 그래프는 finetune_binary_model에 대한 그래프이다.<br> epoch는 전부 50으로 고정시켜 진행했다. 
<div style="width:400px; height:200px; float:left; margin-right:10px">
 <img src="/assets/img/batch8_epoch50.png" width="400px" height="200px">
 <figcaption style="text-align:center">fig3)batch8_epoch50</figcaption>
</div>
<div style="width:400px; height:200px; float:left;">
 <img src="/assets/img/batch64_epoch50.png" width="400px" height="200px">
 <figcaption style="text-align:center">fig4)batch64_epoch50</figcaption>
</div><div style="clear:both;"></div><br>

<div style="width:400px; height:200px; float:left; margin-right:10px">
 <img src="/assets/img/ft_batch8_epoch50.png" width="400px" height="200px">
 <figcaption style="text-align:center">fig5)ft_batch8_epoch50</figcaption>
</div>
<div style="width:400px; height:200px; float:left;">
 <img src="/assets/img/ft_batch64_epoch50.png" width="400px" height="200px">
 <figcaption style="text-align:center">fig6)ft_batch64_epoch50</figcaption>
</div><div style="clear:both;"></div><br>

3) Batch size 16, Epoch 50으로 고정하고 두 모델을 비교해보자.
<div style="width:400px; height:200px; float:left; margin-right:10px;">
 <img src="/assets/img/batch16_epoch50.png" width="400px" height="200px">
 <figcaption style="text-align:center">batch16_epoch50</figcaption>
</div>
<div style="width:400px; height:200px; float:left;">
 <img src="/assets/img/ft_batch16_epoch50.png" width="400px" height="200px">
 <figcaption style="text-align:center">ft_batch16_epoch50</figcaption>
</div><div style="clear:both;"></div><br>

- 왼쪽 그래프는 train_binary_model이며, 오른쪽이 finetune_binary_model이다. Batch size와 Epoch를 고정하고 두 모델을 비교했을 때, accuracy의 fluctuation이 적은 것은 train_binary_model이며, batch size를 8부터 64까지 테스트 했을 때 epoch 50으로는 상대적으로 train_binary_model의 accuracy가 미세한 차이로 더 높게 나왔다.<br> 이것은 Pre-trained model의 fully-connected layer 이전 layer의 weight를 그대로 가져오고, fully-connected layer 부분의 weight만 조정하는 것이기에 작은 epoch 값에서는 accuracy 변동 폭이 작고, 더 좋은 accuracy를 가지는 것으로 생각된다.<br>

- 그렇다면 train_binary_model이 finetune model에 비해 좋은 것일까? 아래 그래프를 살펴보자.
<div style="width:400px; height:200px; float:left; margin-right:10px">
 <img src="/assets/img/batch16_epoch50.png" width="400px" height="200px">
 <figcaption style="text-align:center">batch16_epoch50</figcaption>
</div>
<div style="width:400px; height:200px; float:left;">
 <img src="/assets/img/batch16_epoch100.png" width="400px" height="200px">
 <figcaption style="text-align:center">batch16_epoch100</figcaption>
</div><div style="clear:both;"></div><br>

- 그래프에서 볼 수 있다시피 train_binary_model은 training 횟수가 증가할수록 validation accuracy는 90% 근방에서 머무르고 train accuracy만 증가하게 되는, overfitting 현상이 발생하게 된다.<br> epoch50에서보다 epoch100에서 train accuracy와 validation accuracy의 차이뿐만 아니라 loss 값의 차이도 점점 더 벌어지고 있음을 관찰할 수 있다.<br> 혹시 현재 batch size에서만 발생하는 상황일 수도 있으므로, batch size를 8,16,32,64,256까지 변화해가며 관찰했고 결과는 역시 위 그래프와 비슷한 양상을 보였다.<br><br>

---------------------
## <a name="Discussion">6. Discussion</a> <br><br>
------------------------

-----------------------
## <a name="Reference">7. Reference</a>
-----------------------
