---
layout: post
title:  "AIXDL - Project"
summary: "차량 공유 서비스의 차량 파손 여부를 CNN을 활용하여 판단"
author: taehun
date: '2020-12-02 16:53:00 +0900'
category: Python
toc : true
toc_sticky : true
thumbnail: /assets/img/posts/thumbnail-1.png
keywords: AI+X:DeepLearning, Dataset, VGGNet, Pre-trained model, ImageNet
permalink: /blog/project
---
----------
## **Index**
----------
  
<a href="#Introduction">**1. Introduction**</a>
- 필요성
- 목표<br><br>
 
<a href="#Dataset">**2. Dataset**</a>
- Gate1. Dataset
- Gate2. Dataset <br><br>
 
<a href="#VGGNet">**3. CNN / VGG16**</a>
- CNN
- VGG16 <br><br>

<a href="#Code">**4. Code Analysis**</a> <br><br>
- <a href="#Gate1">Gate1. 이미지 데이터의 차량 여부 판단(Pre-trained VGG16 model)</a> <br>

- <a href="#Gate2">Gate2. 차량의 파손 여부 판단(Transfer Learning & Fine Tuning)</a> <br><br>

<a href="#Evaluation">**5. Evaluation**</a>
- Gate1. Evaluation
- Gate2. Evaluation <br><br>

<a href="#Discussion">**6. Discussion**</a> <br><br>

<a href="#Reference">**7. Reference**</a> <br><br>

-----------
## <a name="Introduction">1. Introduction</a>
-----------

#### - 필요성<br><br>
 - 미래 모빌리티 산업에서 공유 서비스는 핵심 개념 중 하나이다. 대표적인 예로 '쏘카', '그린카' 등의 차량 공유 서비스뿐만 아니라, '따릉이','beam','씽씽'과 같은 자전거, 전동 킥보드의 공유 서비스도 있다. 공유서비스의 대상인 차량, 킥보드 등은 단독으로 소유하는 소유물이 아니기에, 파손 및 손상 여부에 대한 과실 판정을 제 때에, 정확하게 할 필요가 있다.<br><br> 렌트카의 경우 대여 및 반납시 대여자가 차량의 파손여부를 직접 체크한다. 차량 확인 과정에서 발견하지 못한, 즉 대여자의 과실이 아닌 손상으로 인해 발생하는 문제를 확실히 해결하기 위해서는 적절한 시스템이 필요할 것이다. 이 문제점을 해결하기 위해서 딥러닝의 이미지 분류를 이용해서 해결해보고자 한다.<br><br>

#### - 목표<br><br>
 - 본 프로젝트에서는 딥러닝의 Image Classification을 활용해서 문제를 해결하고자 한다. 임의의 이미지 데이터를 입력했을 때 차량인지 아닌지를 판단하고, 만약 차량이 맞다면 차량의 파손 유무를 판단하는 것이 프로젝트의 최종 목표이다. <br><br> 판단은 2개의  Gate를 통해 이루어진다. Gate1에서는  Pre-trained model(VGGNet )을 활용하여 차량 여부 및 차량의 종류를 판단하고, Gate2에서는 직접 학습시킨 모델로  파손 여부를 판단한다.<br><br>
 
<div style="width:700px; height:300px;">
  <center>
    <img src="/assets/img/Gate.png" width="700px" height="300px">
  </center>
  <figcaption style="text-align:center">모델 알고리즘 개략도</figcaption>
</div><br><br>
 
----------------
## <a name="Dataset">2. Dataset</a>
----------------

#### - Gate1. Dataset<br><br>
- **Data1 폴더 : 차량 사진 1000장**<br>(Damaged Car 500, Undamaged Car 500)<br><br>
<center>
  <img src="/assets/img/Gate1_dataset.png" width="600px" height="300px">
  </center>
  <br><br>

- **Gate1 Test용 dataset**<br>(Car 200, Not_Car 200(Transportation 50, Animal 50, Mechanical Stuff 50, etc 50))<br><br>
<center>
  <img src="/assets/img/Gate1_test1.jpg" width="400px" height="200px"><br>
  <img src="/assets/img/Gate1_test2.jpg" width="400px" height="200px">
  </center>
  <br><br>

#### - Gate2. Dataset<br><br>
- **Training data:  차량 사진 600장 (Damaged Car 300, Undamaged Car 300)**
- **Validation data: 차량 사진 400장 (Damaged Car 200, Undamaged Car 200)**

<br><br>

-------------------
## <a name="VGGNet">3. CNN / VGG16</a>
-------------------
<h4>- CNN(Convolutional Neural Network, 합성곱신경망)</h4><br>

- CNN에 대해 이해하기 전 꼭 알아야할 개념은 필터와 이미지데이터의 구조이다. 필터는 그 특징이 해당 데이터에 있는지 없는지를 검출하는 함수다. 이미지 데이터는 가로 x 세로x rgb채널로 이루어진 3차원 구조다.<br><br>필터링은 행렬의 형태로 표현된 이미지에 대해, 행렬로 표현된 필터를 동일하게 적용함으로써 수행된다. CNN은 이러한 필터링 기법을 인공신경망에 적용함으로써 이미지의 공간정보를 유지한 상태로 학습이 가능하도록 만든 모델이다. <br><br>CNN은 기본적으로 1)특징을 추출하는 Convolutional Layer - Max Pooling Layer 블록과 2)분류 학습을 하는 Fully Connected Layer 블록 순서로 진행 된다.<br><br>

<center>
  <img src="/assets/img/CNN1.png" width="700px" height="300px">
  </center>
  <br><br>

- Convolution Layer : CNN의 가장 핵심이 되는 Convolutional Layer에서는 이미지를 Classification 하는데 필요한 특징(Feature) 정보들을 뽑아낸다.  Convolution Filter는 전체 이미지 위에서 차례대로 움직이면서 특징 값을 추출해내며, 특정 Filter의 모양과 일치할 경우 해당 부분에서 큰 값을 갖게 된다. Filter가 전체 이미지를 순회하고 나면 전체 이미지에서 해당 Filter와 유사한 모양을 가진 부분에 대한 Feature들만 추출할 수 있다.<br><br> 

- Max Pooling Layer : Max Pooling Layer에서는 Convolution Filter를 거친 결과로 얻은 각각의 Feature Map에서 특정영역을 형성하여 해당 영역 내에서 가장 큰 값을 도출한다. 그리고 이러한 방법을 Max Pooling이라고 한다.<br><br>

- Fully Connected Layer : 위의 과정을 통해 이미지의 Feature를 뽑다보면 마지막에는 물체와 유사한 형태들의 Feature Map들이 선별된다. Convolution Filter를 통해 시각정보를 최대한 보존해 오면서 마지막 Feature Map들을 일렬로 늘리고, 이들을 입력차원으로 받아들인 후에 하나의 Hidden Layer를 거쳐 Classification 문제를 해결하게 된다.<br><br>

<center>
  <img src="/assets/img/CNN2.png" width="700px" height="300px">
  </center>
  <br><br>

<h4>- VGG16</h4><br><br>
<img src="/assets/img/VGGNet.png" width="700px" height="300px">
<img src="/assets/img/VGGNet2.png" width="700px" height="150px"><br><br>

- VGG16은 CNN모델의 한 종류로, ImageNet 의 이미지 데이터셋 (1000개 클래스에 해당하는 1400만개 이미지 데이터셋)을 분류하는 대회인 ILSVRC-2014에서 92.7% 정확도로 우수한 성과를 거둔 모델이다.<br><br> 13층의 Convolutional Layer와 3층의 Fully Connected layer를 연결하여 총 16층의 Layer로 구성된다.<br>VGG는 다른 CNN모델과는 다르게, 깊이에 따른 변화를 비교하기 위해 모든 Convolution Layer에 3x3 크기의 Filter와, 1이즈의  ZeroPadding을 적용하여 이미지의 크기를 유지한 채 Convolution Layer를 중첩해서 쌓고, Max Pooling 레이어에서 2x2사이즈 kernel, stride=2를 적용하여 이미지를 절반으로 줄여주는 과정을 반복한다.<br><br>VGGNet은 Convolution Layer의 필터 크기를 작게하고 레이어를 늘렸기 때문에 ReLU 비선형을 더 많이 사용할 수 있으며, 이는 Decision Function이 더 잘 학습될 수 있음을 의미한다.<br><br>이런 레이어 구성 방식을 통해 신경망 깊이를 깊게 구성하여 높은 정확도를 얻을 수 있는 것이다.<br><br>

-----------------
## <a name="Code">4. Code Analysis</a>
-----------------

#### <a name="Gate1">Gate1. 이미지 데이터의 차량 여부 판단(Pre-trained VGG16 model)</a><br><br>
<figure>
  <blockquote>
    <p>- Gate1 과정은 파손이 없는 차량 이미지 500개와 파손 차량 이미지 500개, 총 1000장의 차량 사진을 통해 '차량'이라는 것을 인식하는 과정이다.<br><br> Pre-trained VGG16 model의 weight를 사용하는데, 이 모델을 사용하여 이미지 데이터를 넣으면 '버스', '미니밴', '리무진', '트럭'이 출력된다.<br><br> 하지만 Gate2, 그리고 추가적인 과정을 위해서는 단순히 '차량'으로 인식하기만 하면 되므로 '버스', '미니밴', '리무진' 등의 세부 class들을 '차'라는 큰 범주로 출력하는 과정이 Gate1이다.</p>
  </blockquote>
  </figure>

<br><br>
##### **1)get_predictions(preds,top=5) function**<br><br>

```python
def get_predictions(preds, top=5):
    global CLASS_INDEX
    if len(preds.shape) != 2 or preds.shape[1] != 1000:
        raise ValueError('`decode_predictions` expects '
                         'a batch of predictions '
                         '(i.e. a 2D array of shape (samples, 1000)). '
                         'Found array with shape: ' + str(preds.shape))
    if CLASS_INDEX is None:
        fpath = get_file('imagenet_class_index.json',
                         CLASS_INDEX_PATH,
                         cache_subdir='models')
        CLASS_INDEX = json.load(open(fpath))
    results = []
    for pred in preds:
        top_indices = pred.argsort()[-top:][::-1]
        result = [tuple(CLASS_INDEX[str(i)]) + (pred[i],) for i in top_indices]
        list_A = []
        for k in range(top):
        	list_A.append(result[i][1:])
        result.sort(key=lambda x: x[2], reverse=True)
        results.append(result)
    return results
```
```python
vgg16 = VGG16(weights='imagenet')

y = prepare_image('image_file.jpg')
preds = vgg16.predict(y)
print(get_predictions(preds, top=5))
```
- get_predictions(preds, top=5) 결과<br><br>
<figure>
  <blockquote>
    <p>[('racer', 0.6540158), ('sports_car', 0.26624563), ('convertible', 0.028868116), ...]</p>
  </blockquote>
  </figure>
<br>

- 변수 preds는 data1 폴더의 dataset을 resizing 후 VGG16(weight='imagenet')으로 예측한 결과물이며, 1000개의 class에 대한 확률값을 나타내기에 차원은 (1,1000)이다.
- get_prediction 함수는 예측한 class 중 상위 5개의 class name과 확률값을 반환한다.<br><br>

##### **2)get_car_categories() function**<br><br>
```python
def get_car_categories():
    d = defaultdict(float)
    img_list = os.listdir("data1")
    for i, img_path in enumerate(img_list):
        img = prepare_image(root_dir + "data1/"+ img_path)
        out = vgg16.predict(img)
        top = []
        # for i in range(5):
        # 	top.append(get_predictions(out,top=5)[0][i][1:])
        top = get_predictions(out, top=5)
        for j in top[0]:
            d[j[0:2]] += j[2]
            #d[j[1:2]] += j[2]
        if i % 100 == 0:
            print(i, "/" , len(img_list), "complete")
    print("Done\n")
    return Counter(d)
```  
<br>
<figure>
  <blockquote>
    <p>- 해당 디렉토리에 있는 input 이미지에 대해 top5 class와 probability를 구해서 누적 합산하는 함수.<br>- top[0]에는 3가지 성분이 있음. 1000개 class중 고유 이름(j[0] : ex)'n5324', j[1] : class의 이름(label) ex)'sports car', j[2] : 확률값 ex)0.938402)<br>- 결국, 디렉토리 내에 있는 이미지 파일들 중 가장 빈번하게 등장하는 class들을 찾기 위함</p>
  </blockquote>
  </figure>
      
<br><br>

```python
car_categories_all = get_car_categories()
car_categories = [k for k, v in car_categories_all.most_common()[:50]]
```

- get_car_categories로 얻은 list중 key값이 큰 순서대로 상위 50개를 선택하여 car_categories에 저장<br><br>

<figure>
  <blockquote>
    <p>[('minivan',129.24480718624545), ('sports_car', 85.58487360691652), ('convertible', 71.450213502687), ('pickup', 62.12108225026168), ...]</p>
    </blockquote>
      <figcaption style="text-align:center">위와 같은 형태로 50개가 출력됨</figcaption>
      </figure>
    
<br><br>

##### **3)evaluate_car_categories(car_categories) function**<br><br>
```python
def evaluate_car_categories(car_categories):
    img_list = os.listdir("data1")
    num = 0
    failed_img = []
    for i, img_path in enumerate(img_list):
        img = prepare_image("data1/"+img_path)
        out = vgg16.predict(img)
        if i==1: print(out.shape)
        top = get_predictions(out, top=5)
        for j in top[0]:
            if j[0:2] in car_categories:
                num += 1
                break # breaks out of for loop if one of top 50 categories is found
            else:
                pass
            failed_img.append(img_path) # appends to "bad list" if none of the 50 are found
        if i % 100 == 0:
            print(i, "/", len(img_list), "complete")
    failed_img = [k for k, v in Counter(failed_img).items() if v == 5]
    return num, failed_img
```
<br>
- evaluate_car_categories 함수는 위에서 한번 분류한 car_categories를 가지고 failed_img를 찾아내는 함수
- input image에 대한 top5 prediction 중 어느 것도 car_categories안에 포함되지 않는다면 failed_img 리스트에 저장된다.<br>

```python
print(failed_img)
```
<br>

<figure>
  <blockquote>
    <p>['1044.jpg', '1205.jpg', '1102.jpg', '1354.jpg']</p>
  </blockquote>
  </figure>
  
<br><br>
- **Failed_img 확인**<br>
<div style="width:200px; height:300px; float:left; margin-right:10px;">
 <img src="/assets/img/failed_img1.png" width="200px" height="300px">
</div>
<div style="width:200px; height:300px; float:left;">
 <img src="/assets/img/failed_img2.png" width="200px" height="300px">
</div><div style="clear:both;"></div><br>

<figure>
  <blockquote>
    <p>- 총 1,000개의 dataset에서 4개의 경우만 차량으로 인식하지 못한 것이므로 높은 정확도를 가진다.<br>- 하지만 극히 드물게 failed_img와 같이 여러 사물이 있는 경우, 혹은 차량의 일부만 나온 경우는 인식하지 못하는 경우가 있다.<br>--> **Pre-trained VGG16을 이용해서 Nomal/Damaged car image 모두 높은 확률로 차량을 구별할 수 있다.**</p>
  </blockquote>
  </figure>    
    
<br><br>

#### <a name="Gate2">Gate2. 차량의 파손 여부 판단(Transfer Learning & Fine Tuning)</a><br><br>

<figure>
  <blockquote>
    <p>- 코드 설명 전에, Gate2에는 train_binary_model과 finetune_binary_model이 존재한다.<br><br> 첫번째로,train_binary_model은 convolutional layer는 pre-trained VGG16 model을 따르고, bottleneck features를 이용하여 fully-connetecd layer 부분의 weight만 조정해서 training을 진행하는 모델이다.<br><br> 두번째로 fintune model은 pre-trained VGG16 model의 weight는 이용하되, train_binary_model에서 training을 거친 fully-connected layer의 weight를 가져와서, fully-connetecd layer뿐만 아니라 전체 layer에 대한 weight값을 dataset에 맞게 조정하는 모델을 의미한다.</p>
  </blockquote>
  </figure>
      
 <br>


##### **1) load_vgg16() function**<br><br>
  
```python
def load_vgg16():
    vgg16 = VGG16(weights='imagenet')
    model = Sequential()
    model.add(ZeroPadding2D((1,1),input_shape = (img_width, img_height,3)))
    model.add(Conv2D(64, (3, 3), padding = 'same', activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Conv2D(64, (3, 3), padding = 'same',activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))

    ...

    model.add(ZeroPadding2D((1,1)))
    model.add(Conv2D(512, (3, 3), padding = 'same',activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Conv2D(512, (3, 3), padding = 'same',activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Conv2D(512, (3, 3), padding = 'same',activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))
    
    #Transfer Learning 과정.
    model_conv_layer=[]
    vgg16_conv_layer=[]

    for j in range(len(model.layers)):
      if str(model.layers[j]).find("Conv2D")!=-1:
        model_conv_layer.append(j)

    for k in range(len(vgg16.layers)):
      if str(vgg16.layers[k]).find("Conv2D")!=-1:
        vgg16_conv_layer.append(k)

    for j, k in zip(model_conv_layer,vgg16_conv_layer):
      model.layers[j].set_weights(vgg16.layers[k].get_weights())
      
    return model
```
<br>
- VGG16 model은 Convolutional layer와 Fully connected layer를 합쳐서 16-layers이며, Padding, Pooling layer의 배치로 다양한 모델을 만들 수 있다.
- 아래 Transfer Learning 과정은 Pre-trained VGG16 model에서 Convolutional layer의 weight를 load_vgg16()을 통해 쌓은 model의 Convolutional layer에 weight를 인가하는 과정이다.<br>

##### **2) train_binary_model(batch_size) function**<br><br>
  
```python
def train_binary_model(batch_size):

    train_data = np.load(open(location+"/bottleneck_features_train.npy",'rb'))
    train_labels = np.array([0] * train_samples[0] + 
                            [1] * train_samples[1])

    validation_data = np.load(open(location+"/bottleneck_features_validation.npy",'rb'))
    validation_labels = np.array([0] * validation_samples[0] + 
                                 [1] * validation_samples[1])
    # batch_size = 16
    
    model = Sequential()
    model.add(Flatten(input_shape=train_data.shape[1:])) # 512, 4, 4
    print(model.summary())
    model.add(Dense(256, activation = 'relu', kernel_regularizer=l2(0.001)))
    model.add(Dropout(0.5)) 
    model.add(Dense(1, activation = 'sigmoid'))

    model.compile(optimizers.SGD(lr=0.0001, momentum=0.9),
              loss='binary_crossentropy', metrics=['accuracy'])

    
    checkpoint = ModelCheckpoint(top_model_weights_path, monitor='val_accuracy', 
                                 verbose=1, save_best_only=True, save_weights_only=True, mode='auto')

    fit = model.fit(train_data, train_labels,
                    epochs = nb_epoch, batch_size = batch_size,
                    validation_data = (validation_data, validation_labels),
                    callbacks=[checkpoint])

    with open(location+'/top_history.txt', 'w') as f:
        json.dump(fit.history, f)
    
    return model, fit.history
```

- bottleneck_features는 VGG16 model에서 Fully-connected layer로 넘어가기 바로 전 Feature map을 의미한다.
- load_vgg16으로 만든 모델에 fully-connected layer를 연결하고 이 부분을 optimizer와 cross_entropy를 통해 학습시키는 것이다.<br><br>

##### **3) finetune_binary_model(batch_size) function**<br><br>

```python
def finetune_binary_model(batch_size):
    model = load_vgg16()

    # build a classifier model to put on top of the convolutional model
    top_model = Sequential()
    top_model.add(Flatten(input_shape=model.output_shape[1:]))
    top_model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.01)))
    top_model.add(Dropout(0.5))
    top_model.add(Dense(1, activation='sigmoid'))

    top_model.load_weights(top_model_weights_path) # load weights_path

    # add the model on top of the convolutional base
    model.add(top_model)
    
    # set the first 25 layers (up to the last conv block)
    # to non-trainable - weights will not be updated
    for layer in model.layers[:25]:
        layer.trainable=False

    # compile the model with a SGD/momentum optimizer 
    # and a very slow learning rate
    model.compile(loss='binary_crossentropy',
                 optimizer = optimizers.SGD(lr=0.00001, momentum=0.9),
                  metrics=['accuracy'])
    
    # prepare data augmentation configuration
    train_datagen = ImageDataGenerator(rescale=1./255,
                                       rotation_range=40,
                                       width_shift_range=0.2,
                                       height_shift_range=0.2,
                                       shear_range=0.2,
                                       zoom_range=0.2,
                                       horizontal_flip=True,
                                       fill_mode='nearest')

    test_datagen = ImageDataGenerator(rescale=1./255)

    train_generator= train_datagen.flow_from_directory(train_data_dir,
                                                     target_size=(img_height, img_width),
                                                     batch_size=batch_size,
                                                     class_mode='binary')

    validation_generator = test_datagen.flow_from_directory(validation_data_dir,
                                                           target_size=(img_height, img_width),
                                                           batch_size=batch_size,
                                                           class_mode='binary')
                                                           
    checkpoint = ModelCheckpoint(fine_tuned_model_path, monitor='val_accuracy', 
                                 verbose=1, save_best_only=True, 
                                 save_weights_only=False, mode='auto')
    steps_tr = nb_train_samples / batch_size
    steps_val = nb_validatioan_samples / batch_size
    # fine-tune the model
    fit = model.fit_generator(train_generator,
                              steps_per_epoch=steps_tr,
                              epochs=nb_epoch,
                              validation_data=validation_generator,
                              validation_steps=steps_val,
                              verbose=1,
                              callbacks=[checkpoint])
  
    with open(location+'/ft_history.txt', 'w') as f:
        json.dump(fit.history, f)
    
    return model, fit.history
```
<br>
###### **4) evaluate_binary_model(model, directory, labels, batch_size) function**<br><br>
```python
def evaluate_binary_model(model, directory, labels, batch_size):
    datagen = ImageDataGenerator(rescale=1./255)
    generator = datagen.flow_from_directory(directory,
                                target_size=(img_height, img_width),
                                batch_size=batch_size,
                                class_mode='binary', # categorical for multiclass
                                shuffle=False)
    steps_ev = len(labels)/batch_size
    predictions = model.predict_generator(generator, steps_ev)
    
    # use for multiclass
    # pred_labels = np.argmax(predictions, axis=1)
    
    pred_labels = [0 if i <0.5 else 1 for i in predictions]

    print(classification_report(validation_labels, pred_labels))
    cm = confusion_matrix(validation_labels, pred_labels)
    sns.heatmap(cm, annot=True, fmt='g');
```
<br><br>

-----------------
## <a name="Evaluation">5. Evaluation</a>
-----------------
#### - Gate1. Evaluation
<br><br>
#### - Gate2. Evaluation<br><br>

- 먼저, 최종 모델을 확정하기 전에 성능에 영향을 끼치는 hyperparameter를 조정해보자.<br>이 프로젝트에서 조정할 수 있는 hyperparameter는 epoch, batch size, learning rate, activation function, optimizer의 종류, momentum 등이 있다.<br>하지만 여기서는 epoch와 batch size만 조정하면서 테스트 해보았다. <br><br>
<h5>　 1) Epoch</h5>
- 먼저 아래 train_binary_model의 epoch에 따른 그래프를 비교해보자.
<div style="width:400px; height:200px; float:left; margin-right:10px">
 <img src="/assets/img/batch16_epoch50.png" width="400px" height="200px">
 <figcaption style="text-align:center">fig1. batch16_epoch50</figcaption>
</div>
<div style="width:400px; height:200px; float:left;">
 <img src="/assets/img/batch16_epoch100.png" width="400px" height="200px">
 <figcaption style="text-align:center">fig2. batch16_epoch100</figcaption>
</div><div style="clear:both;"></div><br><br>

- 위 그래프는 batch를 16으로 고정하고 epoch를 50, 100으로 변화시킨 그래프이다. <br>위 그래프뿐만 아니라 여러 batch size에서 epoch에 따른 변화를 관찰 했을 때 train binary model은 epoch가 20만 돼도 충분한 accuracy를 가지며, 그 이상으로 진행할 경우 train accuracy와 validation accuracy의 차이가 벌어지는, overfitting이 발생하는 것을 볼 수 있다.<br> 결과적으로 train binary model에서는 epoch = 20으로도 90% 정도의 accuracy를 갖는 결과를 낸다.

<br><br>

<h5>　 2) Batch size</h5>
- fig3, fig4는 train_binary_model에 대한 그래프이고, fig5, fig6은 finetune_binary_model에 대한 그래프이다.<br> epoch는 전부 50으로 고정시켜 진행했다. 
<div style="width:400px; height:200px; float:left; margin-right:10px">
 <img src="/assets/img/batch8_epoch50.png" width="400px" height="200px">
 <figcaption style="text-align:center">fig3. batch8_epoch50</figcaption>
</div>
<div style="width:400px; height:200px; float:left;">
 <img src="/assets/img/batch256_epoch50.png" width="400px" height="200px">
 <figcaption style="text-align:center">fig4. batch256_epoch50</figcaption>
</div><div style="clear:both;"></div><br>

<div style="width:400px; height:200px; float:left; margin-right:10px">
 <img src="/assets/img/ft_batch8_epoch50.png" width="400px" height="200px">
 <figcaption style="text-align:center">fig5. ft_batch8_epoch50</figcaption>
</div>
<div style="width:400px; height:200px; float:left;">
 <img src="/assets/img/ft_batch64_epoch50.png" width="400px" height="200px">
 <figcaption style="text-align:center">fig6. ft_batch64_epoch50</figcaption>
</div><div style="clear:both;"></div><br>


- batch size는 한 번의 iteration에 들어갈 data의 크기를 의미한다. 즉, (전체 Dataset의 개수) = (batch_size) × (iteration)인 것이다.<br><br>
- train_binary_model은 batch_size가 커짐에 따라 overfitting이 더 줄어드는 양상을 보인다. <br>Batch size와 Overfitting의 관계를 검색해본 결과, batch size가 작아질수록 regularization의 효과를 가져와서 overfitting의 확률이 줄어든다고 하나, 이 모델에 대해서는 반대 양상을 나타내어, 추가적인 분석이 필요할 것 같다.<br> 또한, batch size가 증가함에 따라 대부분의 경우 validation loss 값은 소폭 상승했으며, accuracy의 flucuation도 작음을 볼 수 있다.<br><br>
- Finetune_binary_model의 경우도 마찬가지로 batch size가 증가함에 따라 validation loss 값은 상승하는, 같은 양상을 보였다. <br> 반면, validation accuracy의 최대값에 관해서는 batch size가 증가함에 따라 소폭 하락했다.

<br><br>

<h5>3) 동일한 조건으로 Batch size, Epoch를 설정하고 두 모델을 비교해보자.</h5>
<div style="width:400px; height:200px; float:left; margin-right:10px;">
 <img src="/assets/img/batch16_epoch50.png" width="400px" height="200px">
 <figcaption style="text-align:center">fig7. batch16_epoch50</figcaption>
</div>
<div style="width:400px; height:200px; float:left;">
 <img src="/assets/img/ft_batch16_epoch50.png" width="400px" height="200px">
 <figcaption style="text-align:center">fig.8 ft_batch16_epoch50</figcaption>
</div><div style="clear:both;"></div><br><br>

```
- fig7은 train_binary_model이며, fig8은 finetune_binary_model이다.<br><br> Batch size와 Epoch를 고정하고 두 모델을 비교했을 때, accuracy의 fluctuation이 적은 것은 train_binary_model이며, 두 모델에 대해 epoch는 50으로 고정하고, batch size를 8부터 64까지 변화시켰을 때 상대적으로 train_binary_model의 accuracy가 미세한 차이로 더 높게 나왔다.<br><br><br> 이것은 Pre-trained model의 fully-connected layer 이전 layer의 weight를 그대로 가져오고, fully-connected layer 부분의 weight만 조정하는 것이기에 작은 epoch 값에서는 accuracy 변동 폭이 작고, 더 좋은 accuracy를 가지는 것으로 생각된다.
```
<br><br>

- 그렇다면 train_binary_model이 finetune model에 비해 좋은 것일까? 아래 그래프를 살펴보자.<br><br>
<div style="width:400px; height:200px; float:left; margin-right:10px">
 <img src="/assets/img/batch8_epoch50.png" width="400px" height="200px">
 <figcaption style="text-align:center">batch8_epoch50</figcaption>
</div>
<div style="width:400px; height:200px; float:left;">
 <img src="/assets/img/batch8_epoch100.png" width="400px" height="200px">
 <figcaption style="text-align:center">batch8_epoch100</figcaption>
</div><div style="clear:both;"></div><br><br>


- 그래프에서 볼 수 있다시피 train_binary_model은 training 횟수가 증가할수록 validation accuracy는 90% 근방에서 머무르고 train accuracy만 증가하게 되는, overfitting 현상이 발생하게 된다.<br><br> epoch50에서보다 epoch100에서 train accuracy와 validation accuracy의 차이뿐만 아니라 loss 값의 차이도 점점 더 벌어지고 있음을 관찰할 수 있다.<br> 혹시 현재 batch size에서만 발생하는 상황일 수도 있으므로, batch size를 8,16,32,64,256까지 변화해가며 관찰했고 결과는 역시 위 그래프와 비슷한 양상을 보였다.<br><br> 결론적으로, training epoch를 더 큰 값으로 설정하게 되면 finetune model의 accuracy는 train binary model과 동등하거나 그 이상의 값을 가지게 될 것이고, overfitting 측면에서도 더 좋은 성능을 가질 것이다.


---------------------
## <a name="Discussion">6. Discussion</a> <br><br>
------------------------

-----------------------
## <a name="Reference">7. Reference</a>
-----------------------
